---
title: "Just giving data exploration"
author: "Toby J"
date: "6 January 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
```

```{r load packages read data, include = F, collapse = T}
## Basic data load
library(tidyverse)
data_folder <- 'data\\just_giving_data'
donations <- paste(data_folder, 'current_donations.csv', sep  = '\\') %>% read_csv %>% unique # better duplicate removal! (by fundraiser?)
fundraisers <- paste(data_folder, 'current_fundraisers.csv', sep  = '\\') %>% read_csv %>% unique
```

##Data preparation
```{r data_downloaded, echo = F, include = T}

#The download data is assumed to be the date of the most recent donation in the data
download_date <- max(donations$donationDate)
print(download_date)
```

```{r data_summary, echo = F, include = F}
#The below aggregation creates a table of fundraising pages with some information about the donations and then generates a number of time difference columns
aggregated_data <- donations %>%
  group_by(pageShortName) %>%
  summarise(total_raised = sum(amount, na.rm = T),
            donation_count = length(amount),
            first_donation_date = min(donationDate),
            first_donation_amount = amount[which(donationDate == min(donationDate))],
            average_donation = mean(amount, na.rm = T)) %>%
  right_join(fundraisers) %>%
  select(charity, pageShortName, donation_count, total_raised, totalEstimatedGiftAid ,fundraisingTarget, eventDate, expiryDate, totalRaisedOffline, totalRaisedOnline, first_donation_date, first_donation_amount) %>% #Reposition select? and rationalise the below columns
  replace_na(list(donation_count = 0)) %>%
  mutate(has_gift_aid = (totalEstimatedGiftAid != 0 & (totalRaisedOffline >0 | totalRaisedOnline >0)),
         event_in_future = eventDate >= download_date,
         has_no_donations = (totalRaisedOnline == 0),
         first_donation_to_expiry = as.integer(difftime(expiryDate, first_donation_date, units = "days")),
         event_to_expiry = as.integer(difftime(expiryDate, eventDate, units = "days")),
         first_donation_to_event = as.integer(difftime(eventDate, first_donation_date, units = "days")),
         first_donation_to_download = as.integer(difftime(download_date, first_donation_date, units = "days")),
         days_since_event= as.integer(difftime(download_date, eventDate, units = "days")),
         event_month = format(eventDate, "%m"), 
         event_year = format(eventDate, "%Y"),
         first_donation_month = format(first_donation_date, "%m"),
         first_donation_year = format(first_donation_date, "%Y"))
```

##Basic data checks
###no gift aid rate
```{r check_no_gift_aid, echo = TRUE}
aggregated_data %>%
  filter((totalRaisedOffline + totalRaisedOnline) >0) %>%
  group_by(charity) %>%
  summarise(count = length(has_gift_aid),
            gift_aid_count = sum(has_gift_aid).
            average_donatio) %>%
  mutate(percentage_gift_aid = (gift_aid_count/count*100))
```
###no donations (before the event) rate
```{r check_no_donations, echo = TRUE}
sum(aggregated_data$has_no_donations)/nrow(aggregated_data)
```
###Can the expiry be before the event?
```{r expiry_before_event, echo = TRUE}
sum(aggregated_data$eventDate > aggregated_data$expiryDate)
```

## Expected rate of page creation by charity

###Expiry is pretty much always > 3 months (13 weeks after the event (the exceptions are probably mistakes)) #Show this distribution
###From creating pages myself:
###London Marathon 2018 for Animal equality - expiry was 6 month in advance and target was 675 by default
###The target is easy to change manually - the exipry date less so (and can be put pre event but not in the past)

```{r expected rate, echo=FALSE}

#The expiry function is a table of weights based on the cumulitive proporiton of pages that are exoected to have expired after a given amount of time from the first donation. We use data from the previous 90 days as this data should be complete.
expiry_function <- aggregated_data %>%
  filter(days_since_event < 90) %>%
  group_by(first_donation_to_expiry) %>%
  summarise(count = length(first_donation_to_expiry)) %>%
  mutate(cum_count = cumsum(count)) %>%
  na.omit %>% #Ignore pages with no donations
  mutate(cumulitive_proportion = cum_count/max(cum_count)) %>%
  mutate(weight = 1/(1-cumulitive_proportion)) %>%
  filter(is.finite(weight))

#This function just performs a lookup on the expiry function table to generate a weight based on how far in the past a page was first donated to. 
get_nearest_weight <- Vectorize(function(days){
  matched_row <- expiry_function %>%
    filter(first_donation_to_expiry > days) %>%
    filter(first_donation_to_expiry == min(first_donation_to_expiry))
  ifelse(nrow(matched_row) == 1, 
         return(matched_row$weight),
         return(max(expiry_function$weight)))
})

#Peforms the lookup
expected_rate_table <- aggregated_data %>%
  filter(!is.na(first_donation_to_download)) %>%
  mutate(nearest_weight = get_nearest_weight(first_donation_to_download))

#Generate a table of expected rate by charity
expected_rate_by_charity <- expected_rate_table %>%
  group_by(charity, first_donation_month, first_donation_year) %>%
  summarise(expected_frequency = sum(nearest_weight)) %>%
  filter(first_donation_year == 2017) %>%
  group_by(charity) %>%
  summarise(monthly_rate = sum(expected_frequency)/12)

#expected rate by min donations
expected_rate_by_charity
```

#Do donations come in in quick succession?
```{r predicting future donations, echo=FALSE}

donations_summary <- donations %>%
  group_by(pageShortName) %>%
  mutate(first_donation_date = min(donationDate),
         first_donation_date = max(donationDate)) %>%
  mutate(time_since_first = as.numeric(difftime(donationDate, first_donation_date, units = "days")),
         last_donation_to_now) %>%
  filter(time_since_first != 0)

#Calculate deciles
percentiles <- seq(0, 1,0.1)
quantile(donations_summary$time_since_first, percentiles)
```
